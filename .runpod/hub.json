{
    "title": "FastDeploy",
    "description": "FastDeploy : Inference and Deployment Toolkit for LLMs and VLMs based on PaddlePaddle.",
    "type": "serverless",
    "category": "language",
    "iconUrl": "https://avatars.githubusercontent.com/u/23534030?s=48&v=4",
    "config": {
      "runsOn": "GPU",
      "containerDiskInGb": 100,
      "gpuIds": "NVIDIA H100 80GB HBM3, NVIDIA A100 80GB PCIe, NVIDIA H100 80GB HBM3,NVIDIA RTX A2000,NVIDIA RTX A5000, NVIDIA RTX A6000, NVIDIA RTX A4000, NVIDIA GeForce RTX 4090, NVIDIA GeForce RTX 3090",
      "gpuCount": 1,

      "allowedCudaVersions": [
        "12.9", "12.8", "12.7", "12.6", "12.5", "12.4", "12.3"
      ],
  
      "presets": [
        {
          "name": "baidu/ERNIE-4.5-0.3B-Paddle",
          "defaults": {
            "MODEL": "baidu/ERNIE-4.5-0.3B-Paddle",
            "ENABLE_V1_KVCACHE_SCHEDULER": "1"
          }
        }
      ],
  
      "env": [
        {
          "key": "MODEL",
          "input": {
            "name": "Model",
            "type": "string",
            "description": "FastDeploy model repo or local path",
            "default": "baidu/ERNIE-4.5-0.3B-Paddle",
            "required": true
          }
        },
        {
          "key": "MAX_CONCURRENCY",
          "input": {
            "name": "Max Concurrency",
            "type": "number",
            "description": "Only required for service deployment, the actual number of connections established by the service, default 512",
            "default": 512,
            "advanced": true
          }
        },
        {
          "key": "MAX_MODEL_LEN",
          "input": {
            "name": "Max model length",
            "type": "number",
            "description": "Default maximum supported context length for inference, default: 2048.",
            "min": 1024,
            "max": 65536,
            "default": 2048,
            "advanced": true
          }
        },
        {
          "key": "TENSOR_PARALLEL_SIZE", 
          "input": {
            "name": "Tensor parallel size",
            "type": "number",
            "description": "Default tensor parallelism degree for model, default: 1.",
            "default": 1,
            "advanced": true
          }
        },
        {
          "key": "DATA_PARALLEL_SIZE",
          "input": {
            "name": "Data parallel size",
            "type": "number",
            "description": "Default data parallelism degree for model, default: 1.",
            "default": 1,
            "advanced": true
          }
        },
        {
          "key": "BLOCK_SIZE",
          "input": {
            "name": "Block size",
            "type": "number",
            "description": "KVCache management granularity (Token count), recommended default: 64.",
            "default": 64,
            "advanced": true
          }
        },
        {
          "key": "MAX_NUM_SEQS",
          "input": {
            "name": "Max num seqs",
            "type": "number",
            "description": "Maximum concurrent number in Decode phase, default: 8.",
            "default": 8,
            "advanced": true
          }
        },
        {
          "key": "TOKENIZER",
          "input": {
            "name": "Tokenizer",
            "type": "string",
            "description": "Tokenizer name or path, defaults to model path.",
            "advanced": true
          }
        },
        {
          "key": "USE_WARMUP",
          "input": {
            "name": "Use warmup",
            "type": "number",
            "description": "Whether to perform warmup at startup, will automatically generate maximum length data for warmup, enabled by default when automatically calculating KV Cache.",
            "default": 1,
            "advanced": true
          }
        },
        {
          "key": "QUANTIZATION",
          "input": {
            "name": "Quantization",
            "type": "string",
            "description": "Model quantization strategy, when loading BF16 CKPT, specifying wint4 or wint8 supports lossless online 4bit/8bit quantization.",
            "options": [
              {
                "label": "BF16",
                "value": "BF16"
              },
              {
                "label": "wint2",
                "value": "wint2"
              },
              {
                "label": "wint4",
                "value": "wint4"
              },
              {
                "label": "wint8",
                "value": "wint8"
              },
              {
                "label": "block_wise_fp8",
                "value": "block_wise_fp8"
              },
              {
                "label": "w4afp8",
                "value": "w4afp8"
              },
              {
                "label": "w8a8",
                "value": "w8a8"
              },
              {
                "label": "w4a8",
                "value": "w4a8"
              },
              {
                "label": "wfp8afp8",
                "value": "wfp8afp8"
              },
              {
                "label": "tensor_wise_fp8",
                "value": "tensor_wise_fp8"
              }
            ],
            "default": "BF16",
            "advanced": true
          }
        },
        {
          "key": "GPU_MEMORY_UTILIZATION",
          "input": {
            "name": "GPU memory utilization",
            "type": "number",
            "description": "GPU memory utilization, default: 0.9.",
            "default": 0.9,
            "advanced": true
          }
        },
        {
          "key": "KV_CACHE_RATIO",
          "input": {
            "name": "KV cache ratio",
            "type": "number",
            "description": "KVCache blocks are divided between Prefill phase and Decode phase according to kv_cache_ratio ratio, default: 0.75.",
            "default": 0.75,
            "advanced": true
          }
        },
        {
          "key": "ENABLE_PREFIX_CACHING",
          "input": {
            "name": "Enable prefix caching",
            "type": "boolean",
            "description": "Whether to enable Prefix Caching, default: False.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "SWAP_SPACE",
          "input": {
            "name": "Swap space",
            "type": "number",
            "description": "When Prefix Caching is enabled, CPU memory size for KVCache swapping, unit: GB, default: None.",
            "advanced": true
          }
        },
        {
          "key": "ENABLE_CHUNKED_PREFILL",
          "input": {
            "name": "Enable chunked prefill",
            "type": "boolean",
            "description": "Enable Chunked Prefill, default: False.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "MAX_NUM_PARTIAL_PREFILLS",
          "input": {
            "name": "Max num partial prefills",
            "type": "number",
            "description": "When Chunked Prefill is enabled, maximum concurrent number of partial prefill batches, default: 1.",
            "default": 1,
            "advanced": true
          }
        },
        {
          "key": "MAX_LONG_PARTIAL_PREFILLS",
          "input": {
            "name": "Max long partial prefills",
            "type": "number",
            "description": "When Chunked Prefill is enabled, maximum number of long requests in concurrent partial prefill batches, default: 1.",
            "default": 1,
            "advanced": true
          }
        },
        {
          "key": "LONG_PREFILL_TOKEN_THRESHOLD",
          "input": {
            "name": "Long prefill token threshold",
            "type": "number",
            "description": "When Chunked Prefill is enabled, requests with token count exceeding this value are considered long requests, default: max_model_len*0.04.",
            "advanced": true
          }
        },
        {
          "key": "STATIC_DECODE_BLOCKS",
          "input": {
            "name": "Static decode blocks",
            "type": "number",
            "description": "During inference, each request is forced to allocate corresponding number of blocks from Prefill's KVCache for Decode use, default: 2.",
            "default": 2,
            "advanced": true
          }
        },
        {
          "key": "REASONING_PARSER",
          "input": {
            "name": "Reasoning parser",
            "type": "string",
            "description": "Specify the reasoning parser to extract reasoning content from model output.",
            "advanced": true
          }
        },
        {
          "key": "DISABLE_CUSTOM_ALL_REDUCE",
          "input": {
            "name": "Disable custom all-reduce",
            "type": "boolean",
            "description": "Disable Custom all-reduce, default: False.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "USE_INTERNODE_LL_TWO_STAGE",
          "input": {
            "name": "Use internode LL two-stage",
            "type": "boolean",
            "description": "Use two stage communication in deepep moe, default: False.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "DISABLE_SEQUENCE_PARALLEL_MOE",
          "input": {
            "name": "Disable sequence parallel MoE",
            "type": "boolean",
            "description": "Disable sequence parallel moe, default: False.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "SPLITWISE_ROLE",
          "input": {
            "name": "Splitwise role",
            "type": "string",
            "description": "Whether to enable splitwise inference, default value: mixed, supported parameters: [\"mixed\", \"decode\", \"prefill\"].",
            "options": [
              {
                "label": "mixed",
                "value": "mixed"
              },
              {
                "label": "decode",
                "value": "decode"
              },
              {
                "label": "prefill",
                "value": "prefill"
              }
            ],
            "default": "mixed",
            "advanced": true
          }
        },
        {
          "key": "INNODE_PREFILL_PORTS",
          "input": {
            "name": "In-node prefill ports",
            "type": "string",
            "description": "Internal engine startup ports for prefill instances (only required for single-machine PD separation), default: None.",
            "advanced": true
          }
        },
        {
          "key": "GUIDED_DECODING_BACKEND",
          "input": {
            "name": "Guided decoding backend",
            "type": "string",
            "description": "Specify the guided decoding backend to use, supports auto, xgrammar, off, default: off.",
            "options": [
              {
                "label": "auto",
                "value": "auto"
              },
              {
                "label": "xgrammar",
                "value": "xgrammar"
              },
              {
                "label": "off",
                "value": "off"
              }
            ],
            "default": "off",
            "advanced": true
          }
        },
        {
          "key": "GUIDED_DECODING_DISABLE_ANY_WHITESPACE",
          "input": {
            "name": "Guided decoding disable whitespace",
            "type": "boolean",
            "description": "Whether to disable whitespace generation during guided decoding, default: False.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "SPECULATIVE_CONFIG",
          "input": {
            "name": "Speculative decoding config",
            "type": "string",
            "description": "Speculative decoding configuration, only supports standard format JSON string, default: None.",
            "advanced": true
          }
        },
        {
          "key": "DYNAMIC_LOAD_WEIGHT",
          "input": {
            "name": "Dynamic load weight",
            "type": "number",
            "description": "Whether to enable dynamic weight loading, default: 0.",
            "default": 0,
            "advanced": true
          }
        },
        {
          "key": "ENABLE_EXPERT_PARALLEL",
          "input": {
            "name": "Enable expert parallel",
            "type": "boolean",
            "description": "Whether to enable expert parallel.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "ENABLE_LOGPROB",
          "input": {
            "name": "Enable logprob output",
            "type": "boolean",
            "description": "Whether to enable return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.If logrpob is not used, this parameter can be omitted when starting.",
            "default": false,
            "advanced": true
          }
        },
        {
          "key": "LOGPROBS_MODE",
          "input": {
            "name": "Logprobs mode",
            "type": "string",
            "description": "ndicates the content returned in the logprobs. Supported mode: raw_logprobs, processed_logprobs, raw_logits, processed_logits. Raw means the values before applying logit processors, like bad words. Processed means the values after applying such processors.",
            "options": [
              {
                "label": "raw_logprobs",
                "value": "raw_logprobs"
              },
              {
                "label": "processed_logprobs",
                "value": "processed_logprobs"
              },
              {
                "label": "raw_logits",
                "value": "raw_logits"
              },
              {
                "label": "processed_logits",
                "value": "processed_logits"
              }
            ],
            "advanced": true
          }
        },
        {
          "key": "MAX_LOGPROBS",
          "input": {
            "name": "Max logprobs",
            "type": "number",
            "description": "Maximum number of log probabilities to return, default: 20. -1 means vocab_size..",
            "default": 20,
            "advanced": true
          }
        },
        {
          "key": "SERVED_MODEL_NAME",
          "input": {
            "name": "Served model name",
            "type": "string",
            "description": "The model name used in the API. If not specified, the model name will be the same as the --model argument.",
            "advanced": true
          }
        },
        {
          "key": "REVISION",
          "input": {
            "name": "Model revision",
            "type": "string",
            "description": "The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version..",
            "advanced": true
          }
        },
        {
          "key": "CHAT_TEMPLATE",
          "input": {
            "name": "Chat template",
            "type": "string",
            "description": "Specify the template used for model concatenation, It supports both string input and file path input. The default value is None. If not specified, the model's default template will be used..",
            "advanced": true
          }
        },
        {
          "key": "TOOL_CALL_PARSER",
          "input": {
            "name": "Tool call parser",
            "type": "string",
            "description": "Specify the function call parser to be used for extracting function call content from the model's output.",
            "advanced": true
          }
        },
        {
          "key": "TOOL_PARSER_PLUGIN",
          "input": {
            "name": "Tool parser plugin",
            "type": "string",
            "description": "Specify the file path of the tool parser to be registered, so as to register parsers that are not in the code repository. The code format within these parsers must adhere to the format used in the code repository.",
            "advanced": true
          }
        },
        {
          "key": "LOAD_CHOICES",
          "input": {
            "name": "Load choices",
            "type": "string",
            "description": "By default, the \"default\" loader is used for weight loading. To load Torch weights or enable weight acceleration, \"default_v1\" must be used.",
            "options": [
              {
                "label": "default",
                "value": "default"
              },
              {
                "label": "default_v1",
                "value": "default_v1"
              }
            ],
            "default": "default",
            "advanced": true
          }
        },
        {
          "key": "MAX_ENCODER_CACHE",
          "input": {
            "name": "Max encoder cache",
            "type": "number",
            "description": "Maximum number of tokens in the encoder cache (use 0 to disable).",
            "default": 0,
            "advanced": true
          }
        },
        {
          "key": "MAX_PROCESSOR_CACHE",
          "input": {
            "name": "Max processor cache",
            "type": "number",
            "description": "Maximum number of bytes(in GiB) in the processor cache (use 0 to disable).",
            "default": 0,
            "advanced": true
          }
        },
        {
          "key": "API_KEY",
          "input": {
            "name": "API key config",
            "type": "string",
            "description": "Validate API keys in the service request headers, supporting multiple key inputs",
            "advanced": true
          }
        }
      ]
    }
}

